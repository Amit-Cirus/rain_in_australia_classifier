{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib notebook\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = r'C:\\Users\\amitb\\.kaggle'\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "print(\"Kaggle authentication successful!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(r\"C:\\code_projects\\applied_DS\\rain_in_australia_classifier\\weatherAUS.csv\")\n",
    "print(f'There are {data.shape[0]} samples within the dataset')\n",
    "data.head(10)"
   ],
   "id": "84dca29750fd9db7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data.describe(include='all')",
   "id": "7a93a3f4d1557163",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numeric_cols = data.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "\n",
    "for numeric_col in numeric_cols:\n",
    "    data[numeric_col] = data[numeric_col].astype(float)\n",
    "\n",
    "\n",
    "categorical_cols = data.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "categorical_cols.remove('Date')\n",
    "\n",
    "for categorical_col in categorical_cols:\n",
    "    data[categorical_col] = data[categorical_col].astype('category')\n",
    "\n",
    "data['Date'] = data['Date'].astype('datetime64[ns]')\n",
    "\n",
    "data.dtypes"
   ],
   "id": "45e2cd94e061c13c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Handling missing values\n",
   "id": "84aa30a8fccc495f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# checking NA\n",
    "\n",
    "mising_values_col_sum = pd.Series(data.isnull().sum(), name='% of NA').apply(lambda x: f\"{(x / data.shape[0] * 100):.2f}\").astype('float').sort_values(ascending=False)\n",
    "mising_values_col_sum"
   ],
   "id": "4a6ca2c6656fb336",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "features = ['Sunshine', 'Evaporation', 'Cloud3pm','Cloud9am','Pressure9am','Pressure3pm']\n",
    "\n",
    "n_features = len(features)\n",
    "fig, axes = plt.subplots(n_features, 1, figsize=(8, 4*n_features))  # vertical layout\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    axes[i].hist(data[feature], bins=20, color='skyblue', edgecolor='black')\n",
    "    axes[i].set_title(f'{feature} | % missing values {mising_values_col_sum[feature]}')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "bf85fff9c644ad00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# detect features with/without normal distribution\n",
    "\n",
    "\n",
    "results = {'feature': [], 'Skewness': [], 'Excess_Kurtosis': []}\n",
    "\n",
    "for column in features:\n",
    "    x = data[column].dropna()\n",
    "    results['feature'].append(column)\n",
    "    results['Skewness'].append(skew(x))\n",
    "    results['Excess_Kurtosis'].append(kurtosis(x))\n",
    "\n",
    "pd.DataFrame(results)\n",
    "\n"
   ],
   "id": "c6bf2ea364152bba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# use median to imput features with normal distribution\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "features_to_imput_with_median = [\n",
    "    f for f, skew, kurt in zip(results.feature, results.Skewness, results.Excess_Kurtosis)\n",
    "    if abs(skew) < 0.1 and abs(kurt) < 0.5\n",
    "]\n",
    "\n",
    "for feature in features_to_imput_with_median:\n",
    "    feature_median = data[feature].median()\n",
    "    data[feature] = data[feature].fillna(feature_median)"
   ],
   "id": "24611ba1e53b205e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#fillna with random choise to preserves the skewed distribution and extreme values (for Evaporation which dosen't have bi-mode / normal dist)\n",
    "\n",
    "observed = data['Evaporation'].dropna()\n",
    "data['Evaporation'] = data['Evaporation'].fillna(np.random.choice(observed))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(data['Evaporation'].dropna(), bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of Evaporation')\n",
    "plt.xlabel('Evaporation')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ],
   "id": "3a66b8f40861815b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Predict cluster for missing values randomly based on cluster weights for features with bi-modal distribution\n",
    "\n",
    "bi_mode_features = ['Cloud3pm', 'Cloud9am', 'Sunshine']\n",
    "\n",
    "for col in bi_mode_features:\n",
    "    observed = data[col].dropna().values.reshape(-1,1)\n",
    "\n",
    "    # Fit 2-component GMM\n",
    "    gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "    gmm.fit(observed)\n",
    "\n",
    "    # Missing indices\n",
    "    missing_idx = data[col].isna()\n",
    "    n_missing = missing_idx.sum()\n",
    "\n",
    "    # Assign clusters to missing values\n",
    "    sampled_cluster = np.random.choice(2, size=n_missing, p=gmm.weights_)\n",
    "\n",
    "    # Get cluster labels for observed values\n",
    "    cluster_labels = gmm.predict(observed)\n",
    "\n",
    "    # Precompute cluster-wise observed values for faster sampling\n",
    "    cluster_values = [observed[cluster_labels == k].flatten() for k in range(2)]\n",
    "\n",
    "    # Vectorized imputation\n",
    "    imputed_values = np.array([np.random.choice(cluster_values[k]) for k in sampled_cluster])\n",
    "\n",
    "    data.loc[missing_idx, col] = imputed_values\n",
    "\n",
    "\n",
    "missing_after_GaussianMixture = pd.Series(data[bi_mode_features].isna().sum(),name='% missing')\n",
    "missing_after_GaussianMixture\n",
    "\n",
    "n_features = len(bi_mode_features)\n",
    "\n",
    "fig, axes = plt.subplots(n_features, 1, figsize=(8, 4*n_features))  # vertical layout\n",
    "\n",
    "for i, feature in enumerate(bi_mode_features):\n",
    "    axes[i].hist(data[feature], bins=20, color='skyblue', edgecolor='black')\n",
    "    axes[i].set_title(f'{feature} | % missing values: {missing_after_GaussianMixture[feature]}')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "6098c6ecea1189d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#final review of NA values info (for these features with less than 10% missing values\n",
    "\n",
    "cols_na = pd.Series(data.isnull().sum(), name='% of NA').apply(lambda x: f\"{(x / data.shape[0] * 100):.2f}\").astype('float').sort_values(ascending=False)\n",
    "\n",
    "filtered = cols_na[cols_na > 0]\n",
    "\n",
    "features = filtered.index.tolist()\n",
    "\n",
    "results = {'feature': [], 'Skewness': [], 'Excess_Kurtosis': []}\n",
    "\n",
    "for column in features:\n",
    "    if column in numeric_cols:\n",
    "        x = data[column].dropna()\n",
    "        results['feature'].append(column)\n",
    "        results['Skewness'].append(skew(x))\n",
    "        results['Excess_Kurtosis'].append(kurtosis(x))\n",
    "\n",
    "data_results = pd.DataFrame(results)"
   ],
   "id": "8830dd04958afd63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_results",
   "id": "ec8ace50fdf351ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# features_numeric = [x for x in filtered.index.tolist() if x in numeric_cols]\n",
    "#\n",
    "# n_features = len(features_numeric)\n",
    "# fig, axes = plt.subplots(n_features, 1, figsize=(8, 4*n_features))  # vertical layout\n",
    "#\n",
    "# for i, feature in enumerate(features_numeric):\n",
    "#     axes[i].hist(data[feature], bins=20, color='skyblue', edgecolor='black')\n",
    "#     axes[i].set_title(f'{feature}')\n",
    "#     axes[i].set_xlabel(f'{feature}')\n",
    "#     axes[i].set_ylabel('Frequency')\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "id": "6cadd37bb391983d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "normal_features = data_results[\n",
    "    (data_results['Skewness'].abs() < 0.1) & (data_results['Excess_Kurtosis'].abs() < 0.5)\n",
    "]['feature'].tolist()\n",
    "\n",
    "skewed_features = data_results[\n",
    "    ~data_results['feature'].isin(normal_features)\n",
    "]['feature'].tolist()\n",
    "\n",
    "# Impute normal-ish features with mean\n",
    "for feature in normal_features:\n",
    "    data[feature] = data[feature].fillna(data[feature].mean())\n",
    "\n",
    "# Impute highly skewed features with median\n",
    "for feature in skewed_features:\n",
    "    data[feature] = data[feature].fillna(data[feature].median())"
   ],
   "id": "d6807458f5c8cc66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nan_results = pd.Series(data.isnull().sum(), name='% of NA').apply(lambda x: f\"{(x / data.shape[0] * 100):.2f}\").astype('float').sort_values(ascending=False)\n",
    "cols_to_imput = nan_results[nan_results.values >0].index\n",
    "data[cols_to_imput].info()"
   ],
   "id": "501ccaf058504217",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data[cols_to_imput].describe(include='all')\n",
   "id": "dc0ee53eda17363a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "wind_cols = [x for x in cols_to_imput if 'Wind' in str(x)]\n",
    "\n",
    "common_values = set(data[wind_cols[0]].unique())\n",
    "for col in wind_cols[1:]:\n",
    "    common_values &= set(data[col].unique())\n",
    "\n",
    "print(\"Common values across all columns:\", common_values)"
   ],
   "id": "810401d7a099d142",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for col in wind_cols:\n",
    "    data[col]= data[col].fillna(data[col].mode()[0])"
   ],
   "id": "f1c7c76b42de4673",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Adding Features",
   "id": "a7024d360bafca16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#create categorical Season feature\n",
    "\n",
    "def get_season(date):\n",
    "    day = date.day\n",
    "    month = date.month\n",
    "\n",
    "    if (month == 12 and day >= 21) or (month <= 3 and (month < 3 or day <= 21)):\n",
    "        return 'Summer'\n",
    "    elif (month == 3 and day >= 22) or (month <= 6 and (month < 6 or day <= 21)):\n",
    "        return 'Autumn'\n",
    "    elif (month == 6 and day >= 22) or (month <= 9 and (month < 9 or day <= 21)):\n",
    "        return 'Winter'\n",
    "    elif (month == 9 and day >= 22) or (month <= 12 and (month < 12 or day <= 20)):\n",
    "        return 'Spring'\n",
    "\n",
    "data['Season'] = data['Date'].map(get_season)\n",
    "data['Season'] = data['Season'].astype('category')\n",
    "\n",
    "data = pd.get_dummies(data, columns=[\"Season\"], drop_first=False)\n"
   ],
   "id": "fe4d0aa803d6a43c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "binary_dict = {'No': 0, 'Yes':1}\n",
    "\n",
    "data['RainToday']=data['RainToday'].map(binary_dict)\n",
    "data['RainTomorrow']=data['RainTomorrow'].map(binary_dict)"
   ],
   "id": "fc7542e749e132c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#extract COSIN for day, month and year\n",
    "\n",
    "data[\"day_of_week\"] = data[\"Date\"].dt.weekday      # 0 = Monday\n",
    "data[\"month\"] = data[\"Date\"].dt.month\n",
    "data[\"day_of_year\"] = data[\"Date\"].dt.dayofyear\n",
    "data[\"year\"] = data[\"Date\"].dt.year\n",
    "\n",
    "# Cyclical encoding\n",
    "data[\"day_of_week_sin\"] = np.sin(2 * np.pi * data[\"day_of_week\"] / 7)\n",
    "data[\"day_of_week_cos\"] = np.cos(2 * np.pi * data[\"day_of_week\"] / 7)\n",
    "\n",
    "data[\"month_sin\"] = np.sin(2 * np.pi * data[\"month\"] / 12)\n",
    "data[\"month_cos\"] = np.cos(2 * np.pi * data[\"month\"] / 12)\n",
    "\n",
    "data[\"day_of_year_sin\"] = np.sin(2 * np.pi * data[\"day_of_year\"] / 365)\n",
    "data[\"day_of_year_cos\"] = np.cos(2 * np.pi * data[\"day_of_year\"] / 365)\n",
    "\n",
    "data.drop(columns='Date',inplace=True)"
   ],
   "id": "ba0c932f19d25a7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data.dropna(subset='RainTomorrow',inplace=True,axis=0)\n",
    "data['RainTomorrow'].isna().sum()"
   ],
   "id": "65b202a9ccd6ef44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data = pd.get_dummies(data, columns=[\"RainToday\"], drop_first=False)",
   "id": "a87393c194ce8938",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Label encoing for categorical columns\n",
    "\n",
    "Location = LabelEncoder()\n",
    "data['Location'] = Location.fit_transform(data['Location'])\n",
    "\n",
    "wind_categories = sorted(data[wind_cols[0]].unique())  # replace 'wind1' with one of the 3\n",
    "le_wind = LabelEncoder()\n",
    "le_wind.fit(wind_categories)\n",
    "\n",
    "for col in wind_cols:\n",
    "    data[col + '_encoded'] = le_wind.transform(data[col])\n",
    "\n",
    "data.drop(columns=[x for x in data.columns if 'Wind' in x and 'encoded' not in x], inplace=True)"
   ],
   "id": "b463a3225d4f1947",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data.head()",
   "id": "6103f38c3d2330c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# spliting",
   "id": "82120951f7d950a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = data.drop(columns='RainTomorrow')\n",
    "y = data['RainTomorrow']"
   ],
   "id": "7636998e710801ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y.value_counts()",
   "id": "826c7039dc436e6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state = 42)\n",
   "id": "7ffbc61debe7bce6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#detect columns for standard scaler\n",
    "\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "print(numeric_cols)\n",
    "\n",
    "X_train = X_train[['Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine',\n",
    "       'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm','Temp9am', 'Temp3pm']]\n",
    "\n",
    "X_test = X_test[['Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine',\n",
    "       'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm','Temp9am', 'Temp3pm']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply same scaler to test data\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "id": "8dee35caf2a5fe8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def train_xgb_with_test_metrics(X_train, X_test, y_train, y_test, param_grid=None, n_splits=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Train XGBClassifier with Stratified K-Fold grid search.\n",
    "    Evaluate metrics only on held-out test set.\n",
    "    Plots confusion matrix, ROC-AUC, and feature importance.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_test : pd.DataFrame or np.array\n",
    "        Feature matrices for train/test.\n",
    "    y_train, y_test : pd.Series or np.array\n",
    "        Target arrays for train/test.\n",
    "    param_grid : dict\n",
    "        Hyperparameter grid for GridSearchCV.\n",
    "    n_splits : int\n",
    "        Number of folds for StratifiedKFold.\n",
    "    random_state : int\n",
    "        Random seed.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    best_model : fitted GridSearchCV best estimator\n",
    "    \"\"\"\n",
    "\n",
    "    # Default hyperparameter grid\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 5],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'subsample': [0.8, 1],\n",
    "            'colsample_bytree': [0.8, 1],\n",
    "            'gamma': [0, 1]\n",
    "        }\n",
    "\n",
    "    # Stratified K-Fold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # XGBClassifier\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=random_state)\n",
    "\n",
    "    # Grid search\n",
    "    grid = GridSearchCV(\n",
    "        estimator=xgb,\n",
    "        param_grid=param_grid,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        cv=skf,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Fit on training set\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    print(\"Best hyperparameters:\", grid.best_params_)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix (Test Set)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"Classification Report (Test Set):\\n\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # ROC-AUC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    auc_score = roc_auc_score(y_test, y_proba)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr, tpr, label=f'ROC-AUC = {auc_score:.3f}')\n",
    "    plt.plot([0,1],[0,1],'--', color='gray')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (Test Set)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Feature Importance\n",
    "    importance = best_model.feature_importances_\n",
    "    feature_names = X_train.columns if isinstance(X_train, pd.DataFrame) else [f\"f{i}\" for i in range(X_train.shape[1])]\n",
    "    feat_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "    feat_df = feat_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feat_df)\n",
    "    plt.title(\"Feature Importance\")\n",
    "    plt.show()\n",
    "\n",
    "    return best_model, grid\n"
   ],
   "id": "6f5429b871db6f21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best_model, grid = train_xgb_with_test_metrics(X_train_scaled, X_test_scaled, y_train, y_test)",
   "id": "5a6d0972373f8dfc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
